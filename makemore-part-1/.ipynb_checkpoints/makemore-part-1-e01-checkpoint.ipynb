{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd853e5a-8c6d-4975-95c8-62cd8b692345",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercises:\n",
    "# E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "ded8d0ae-8200-40dc-8fde-645036b58c61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"makemore-part-1/names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "6a3abff5-b0ae-44ce-9121-034e17ba525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trigram model below\n",
    "\n",
    "trigram = {}\n",
    "\n",
    "for w in words:\n",
    "    # initializing with .. helps in creating tensor below and initializing with 0 in the loop for populating the next words. Ending does not matter.\n",
    "    w = \"..\" + w + \".\"\n",
    "    for i in range(len(w) - 2):\n",
    "        t = (w[i:i+2], w[i+2])\n",
    "        trigram[t] = trigram.get(t, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "9efa38bd-84b5-4a1c-92c0-0b5f8e1b9af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "# here we have to prepare a tensor such that we store the trigram data model.\n",
    "# the rows are all combinations of \"..\", \".a\", \".b\"...., \"zz\"\n",
    "# the cols are all combinations of \".\", \"a\", \"b\"...., \"z\"\n",
    "\n",
    "import torch\n",
    "import itertools\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "characters = [\".\"] + [chr(c) for c in range(ord('a'), ord('z') + 1)]\n",
    "\n",
    "# Generate all possible two-character combinations for row labels\n",
    "row_labels = sorted(\"\".join(pair) for pair in itertools.product(characters, repeat=2))\n",
    "\n",
    "print(len(row_labels))\n",
    "\n",
    "row_label_map = {label: idx for idx, label in enumerate(row_labels)}\n",
    "\n",
    "col_labels = characters\n",
    "\n",
    "col_label_map = {label: idx for idx, label in enumerate(col_labels)}\n",
    "\n",
    "# creating a tensor of 1 so all combinations have a minimum value and does not result in exceptions while creating the words\n",
    "N = torch.ones((len(row_labels), len(col_labels)), dtype=torch.int32)\n",
    "\n",
    "# Populate the tensor with real values from the trigram\n",
    "for (row_key, col_key), count in trigram.items():\n",
    "    row_idx = row_labels.index(row_key)\n",
    "    col_idx = col_labels.index(col_key)\n",
    "    N[row_idx, col_idx] = N[row_idx, col_idx] + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "6162e7b2-a70c-46be-8771-c45d9680a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nra\n",
      "cjppaprallee\n",
      "lrayah\n",
      "draiah\n",
      "bdon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = torch.Generator().manual_seed(234243422)\n",
    "\n",
    "N = N.float()\n",
    "probs = N / N.sum(dim=1, keepdim=True)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    next_row_label_ix = 0\n",
    "    \n",
    "    while True:\n",
    "        p = probs[next_row_label_ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "        if ix >= len(col_labels):\n",
    "            print(ix)\n",
    "            \n",
    "        out.append(col_labels[ix])\n",
    "        \n",
    "        next_row_label_ix = row_label_map.get(''.join(out[-2:] if len(out) >= 2 else \"..\"))\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "0b156b34-6335-486d-a549-1605fcb800e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll=tensor(2.2120)\n"
     ]
    }
   ],
   "source": [
    "# until this point, we have a model (via probabilities) that generates the next name based on trigram, but we need to find out the loss of such a model\n",
    "# the code in this section finds out the loss\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    # initializing with .. helps in creating tensor below and initializing with 0 in the loop for populating the next words. Ending does not matter.\n",
    "    w = \"..\" + w + \".\"\n",
    "    for i in range(len(w) - 2):\n",
    "        t = (w[i:i+2], w[i+2])\n",
    "        prob = probs[row_label_map[t[0]]][col_label_map[t[1]]]\n",
    "        n += 1\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "\n",
    "# since log likelihood is negative and as general convention, we are making it positive\n",
    "# log likelihood will always be negative because probabilities are less than 1 and the log of all these are going be negative numbers.\n",
    "nll = -log_likelihood\n",
    "\n",
    "# averaging it out\n",
    "nll = nll/n\n",
    "\n",
    "print(f\"{nll=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "cac3eccc-7116-41b4-b6c2-d387381d989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    w = \"..\" + w + \".\"\n",
    "    for i in range(len(w) - 2):\n",
    "        t = (w[i:i+2], w[i+2])\n",
    "        xs.append(row_label_map[t[0]])\n",
    "        ys.append(col_label_map[t[1]])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num_elements = xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "595d8578-aa0f-4022-bf1b-0302d7dfadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6cb5f3b1-815b-4bad-9a0f-5c750def362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(234243422)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "7f5ae683-117d-4949-a2d4-b26613e4f87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3546183109283447\n",
      "2.3546178340911865\n",
      "2.354617118835449\n",
      "2.354616641998291\n",
      "2.354616165161133\n",
      "2.3546154499053955\n",
      "2.354614734649658\n",
      "2.3546142578125\n",
      "2.3546135425567627\n",
      "2.3546133041381836\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    xenc = F.one_hot(xs, num_classes=729).float()\n",
    "    logits = (xenc @ W).exp()  # log counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(dim=1, keepdims=True)\n",
    "\n",
    "    loss = -probs[torch.arange(num_elements), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -0.1*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "877c8d2e-2050-46f4-a33e-f662820f51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrxqooppaphaws\n",
      "gbyrayajadraiah\n",
      "pdrey\n",
      "mriquoanmflon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(234243422)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    next_row_label_ix = 0\n",
    "    \n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([next_row_label_ix]), num_classes=729).float()\n",
    "        logits = (xenc @ W).exp()  # log counts\n",
    "        counts = logits.exp()\n",
    "        p = counts/counts.sum(dim=1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "        if ix >= len(col_labels):\n",
    "            print(ix)\n",
    "            \n",
    "        out.append(col_labels[ix])\n",
    "        \n",
    "        next_row_label_ix = row_label_map.get(''.join(out[-2:] if len(out) >= 2 else \"..\"))\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "38209d99-e9f4-469a-9f19-cd5505253fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "total_dataset_len = len(words)\n",
    "train_dataset_len = int(0.8*total_dataset_len)\n",
    "dev_dataset_len = int(0.1*total_dataset_len)\n",
    "test_dataset_len = total_dataset_len - train_dataset_len - dev_dataset_len\n",
    "\n",
    "train_set, dev_set, test_set = random_split(words, [train_dataset_len, dev_dataset_len, test_dataset_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "62eed49d-7e24-493a-8d40-e8247f230b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now retraining the model with just the training dataset\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in train_set:\n",
    "    w = \"..\" + w + \".\"\n",
    "    for i in range(len(w) - 2):\n",
    "        t = (w[i:i+2], w[i+2])\n",
    "        xs.append(row_label_map[t[0]])\n",
    "        ys.append(col_label_map[t[1]])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num_elements = xs.nelement()\n",
    "\n",
    "g = torch.Generator().manual_seed(234243422)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a02b0f6e-0187-4070-a95e-d79348893609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.418052911758423\n",
      "2.4180514812469482\n",
      "2.4180502891540527\n",
      "2.418048858642578\n",
      "2.4180471897125244\n",
      "2.418046236038208\n",
      "2.4180445671081543\n",
      "2.4180431365966797\n",
      "2.418041944503784\n",
      "2.4180405139923096\n",
      "2.418039083480835\n",
      "2.4180378913879395\n",
      "2.4180362224578857\n",
      "2.418034791946411\n",
      "2.4180335998535156\n",
      "2.418032169342041\n",
      "2.4180305004119873\n",
      "2.418029546737671\n",
      "2.4180281162261963\n",
      "2.418026924133301\n",
      "2.418025255203247\n",
      "2.4180238246917725\n",
      "2.418022632598877\n",
      "2.4180212020874023\n",
      "2.4180195331573486\n",
      "2.418018341064453\n",
      "2.4180171489715576\n",
      "2.418015480041504\n",
      "2.4180142879486084\n",
      "2.4180126190185547\n",
      "2.418011426925659\n",
      "2.4180099964141846\n",
      "2.41800856590271\n",
      "2.4180073738098145\n",
      "2.41800594329834\n",
      "2.4180045127868652\n",
      "2.4180033206939697\n",
      "2.418001890182495\n",
      "2.4180002212524414\n",
      "2.417999029159546\n",
      "2.4179975986480713\n",
      "2.417996406555176\n",
      "2.417994976043701\n",
      "2.4179933071136475\n",
      "2.417992115020752\n",
      "2.4179906845092773\n",
      "2.4179890155792236\n",
      "2.417987823486328\n",
      "2.4179866313934326\n",
      "2.417985439300537\n",
      "2.4179837703704834\n",
      "2.417982578277588\n",
      "2.417980909347534\n",
      "2.4179797172546387\n",
      "2.417978048324585\n",
      "2.4179770946502686\n",
      "2.417975425720215\n",
      "2.4179742336273193\n",
      "2.4179728031158447\n",
      "2.41797137260437\n",
      "2.4179701805114746\n",
      "2.417968511581421\n",
      "2.4179670810699463\n",
      "2.417965888977051\n",
      "2.417964458465576\n",
      "2.4179630279541016\n",
      "2.417961597442627\n",
      "2.4179604053497314\n",
      "2.417958974838257\n",
      "2.417957305908203\n",
      "2.4179563522338867\n",
      "2.417954921722412\n",
      "2.4179534912109375\n",
      "2.417952299118042\n",
      "2.4179506301879883\n",
      "2.4179494380950928\n",
      "2.417947769165039\n",
      "2.4179463386535645\n",
      "2.417945146560669\n",
      "2.4179439544677734\n",
      "2.4179422855377197\n",
      "2.417940855026245\n",
      "2.4179396629333496\n",
      "2.417938232421875\n",
      "2.4179365634918213\n",
      "2.417935371398926\n",
      "2.4179341793060303\n",
      "2.4179327487945557\n",
      "2.417931079864502\n",
      "2.4179298877716064\n",
      "2.417928695678711\n",
      "2.4179270267486572\n",
      "2.4179255962371826\n",
      "2.417924642562866\n",
      "2.4179229736328125\n",
      "2.417921781539917\n",
      "2.4179201126098633\n",
      "2.4179189205169678\n",
      "2.417917490005493\n",
      "2.4179160594940186\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    xenc = F.one_hot(xs, num_classes=729).float()\n",
    "    logits = (xenc @ W).exp()  # log counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(dim=1, keepdims=True)\n",
    "\n",
    "    loss = -probs[torch.arange(num_elements), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -0.1*W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Andrej (venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
